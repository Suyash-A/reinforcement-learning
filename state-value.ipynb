{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 269.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 193.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0 is the left terminal state\n",
    "# 6 is the right terminal state\n",
    "# 1 ... 5 represents A ... E\n",
    "VALUES = np.zeros(7)\n",
    "VALUES[1:6] = 0.5\n",
    "# For convenience, we assume all rewards are 0\n",
    "# and the left terminal state has value 0, the right terminal state has value 1\n",
    "# This trick has been used in Gambler's Problem\n",
    "VALUES[6] = 1\n",
    "\n",
    "# set up true state values\n",
    "TRUE_VALUE = np.zeros(7)\n",
    "TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0\n",
    "TRUE_VALUE[6] = 1\n",
    "\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1\n",
    "\n",
    "# @values: current states value, will be updated if @batch is False\n",
    "# @gamma: step size\n",
    "# @batch: whether to update @values\n",
    "def temporal_difference(values, gamma=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "    rewards = [0]\n",
    "    while True:\n",
    "        old_state = state\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        # Assume all rewards are 0\n",
    "        reward = 0\n",
    "        trajectory.append(state)\n",
    "        # TD update\n",
    "        if not batch:\n",
    "            values[old_state] += gamma * (reward + values[state] - values[old_state])\n",
    "        if state == 6 or state == 0:\n",
    "            break\n",
    "        rewards.append(reward)\n",
    "    return trajectory, rewards\n",
    "\n",
    "# @values: current states value, will be updated if @batch is False\n",
    "# @gamma: step size\n",
    "# @batch: whether to update @values\n",
    "def monte_carlo(values, gamma=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [3]\n",
    "\n",
    "    while True:\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        trajectory.append(state)\n",
    "        if state == 6:\n",
    "            returns = 1.0\n",
    "            break\n",
    "        elif state == 0:\n",
    "            returns = 0.0\n",
    "            break\n",
    "\n",
    "    if not batch:\n",
    "        for state_ in trajectory[:-1]:\n",
    "            # MC update\n",
    "            values[state_] += gamma * (returns - values[state_])\n",
    "    return trajectory, [returns] * (len(trajectory) - 1)\n",
    "\n",
    "# Example 6.2 left\n",
    "def compute_state_value():\n",
    "    episodes = [0, 1, 10, 100]\n",
    "    current_values = np.copy(VALUES)\n",
    "\n",
    "    # Temporal Difference\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(episodes[-1] + 1):\n",
    "        if i in episodes:\n",
    "            plt.plot(current_values, label=str(i) + ' Episodes')\n",
    "        temporal_difference(current_values)\n",
    "    plt.plot(TRUE_VALUE, label='True Values')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Estimated Value')\n",
    "    plt.title('TD(0)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Monte Carlo\n",
    "    current_values = np.copy(VALUES)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(episodes[-1] + 1):\n",
    "        if i in episodes:\n",
    "            plt.plot(current_values, label=str(i) + ' Episodes')\n",
    "        monte_carlo(current_values)\n",
    "    plt.plot(TRUE_VALUE, label='True Values')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Estimated Value')\n",
    "    plt.title('Gamma-Constant Monte Carlo')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 6.2 right\n",
    "def rms_error():\n",
    "    # Same gamma value can appear in both arrays\n",
    "    td_gammas = [0.1]\n",
    "    mc_gammas = [0.1]\n",
    "    episodes = 100 + 1\n",
    "    runs = 100\n",
    "    for i, gamma in enumerate(td_gammas + mc_gammas):\n",
    "        total_errors = np.zeros(episodes)\n",
    "        if i < len(td_gammas):\n",
    "            method = 'TD(0)'\n",
    "            linestyle = 'solid'\n",
    "        else:\n",
    "            method = 'Gamma-Const MC'\n",
    "            linestyle = 'dashdot'\n",
    "        for r in tqdm(range(runs)):\n",
    "            errors = []\n",
    "            current_values = np.copy(VALUES)\n",
    "            for i in range(0, episodes):\n",
    "                errors.append(np.sqrt(np.sum(np.power(TRUE_VALUE - current_values, 2)) / 5.0))\n",
    "                if method == 'TD(0)':\n",
    "                    temporal_difference(current_values, gamma=gamma)\n",
    "                else:\n",
    "                    monte_carlo(current_values, gamma=gamma)\n",
    "            total_errors += np.asarray(errors)\n",
    "        total_errors /= runs\n",
    "        plt.plot(total_errors, linestyle=linestyle, label=method + ', gamma = %.02f' % (gamma))\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('RMS')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def quality_estimate():\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    compute_state_value()\n",
    "\n",
    "    # plt.subplot(2, 1, 2)\n",
    "    # rms_error()\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.savefig('./images/quality_estimate.png')\n",
    "    plt.close()\n",
    "\n",
    "def rms_plot():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(1, 1, 1)\n",
    "    rms_error()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('./images/rms_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #quality_estimate()\n",
    "    rms_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
